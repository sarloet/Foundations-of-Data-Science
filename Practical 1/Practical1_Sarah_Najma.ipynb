{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "id1w7kwZVF3I"
   },
   "source": [
    "# Practical 1 : Implementation of Linear Regression (Ridge, Lasso)\n",
    "\n",
    "This practical has two parts. The first part is to implement and train a linear regression model using the least squares method. We will implement the model and train it on the winequality dataset using the NumPy library. Learning curves will be plotted to determine if the model is overfitting or underfitting.\n",
    "\n",
    "In the second part, we will apply the basis expansion to the dataset and train linear regression models with regularization, i.e., Ridge and Lasso. These tasks do not need to be implemented from scratch -- We will use the functions from the scikit-learn library. During training, we use the validation data to determine the optimal hyperparameters. \n",
    "An optional task is to implement the hyperparameter selection using the k-fold cross-validation method. Five bonus points are awarded for this task. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTZv9o5i4gy3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import _pickle as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1-ZQWqTVPno"
   },
   "source": [
    "## Dataset\n",
    "We will use the winequality dataset for this practical. The dataset is available here:\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine+Quality. \n",
    "In order to make it easier to import the dataset, the dataset has been converted to the numpy array format and shuffled, so that we can start the practical directly. The converted dataset is available on the OLAT page.\n",
    "\n",
    "#### Attribute Information\n",
    "Input variables (based on physicochemical tests):\n",
    "1. fixed acidity\n",
    "2. volatile acidity\n",
    "3. citric acid\n",
    "4. residual sugar\n",
    "5. chlorides\n",
    "6. free sulfur dioxide\n",
    "7. total sulfur dioxide\n",
    "8. density\n",
    "9. pH\n",
    "10. sulphates\n",
    "11. alcohol\n",
    "\n",
    "Output variable (based on sensory data):\n",
    "\n",
    "12. quality (score between 0 and 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzDL9RQiVaPY"
   },
   "source": [
    "There are two files in the dataset, one for white wine data and one for red wine data. We focus on the white wine data, which is the larger dataset. The following code loads the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1423,
     "status": "ok",
     "timestamp": 1596436129238,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "NYkwbebUVO_i",
    "outputId": "80ed8916-85c3-4564-cda8-d8a8f36aaa1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is a matrix with the dimension (4898, 11). That is, 4898 data records and 11 features.\n",
      "y is a vector with 4898 values. They are the labels of the data records in X.\n"
     ]
    }
   ],
   "source": [
    "# load the white wine dataset\n",
    "# X is the feature matrix that stores the feature values of the data records\n",
    "# y is the label vector that stores the labels of the data records\n",
    "X, y = cp.load(open('winequality-white.pickle', 'rb'))\n",
    "\n",
    "# check the size of the data\n",
    "print(\"X is a matrix with the dimension {}. That is, {} data records and {} features.\".format(X.shape, X.shape[0], X.shape[1]))\n",
    "print(\"y is a vector with {} values. They are the labels of the data records in X.\".format(y.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q2yKNR49Wkn8"
   },
   "source": [
    "## Understanding What Weâ€™re Predicting\n",
    "\n",
    "First, let's look at the labels. \n",
    "We make the bar chart below to show the distribution of labels in the dataset. \n",
    "The y-values are integers from 3 to 9. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "4L_JDK3dWrsR",
    "outputId": "71b22bf6-77ce-4bd6-d5b1-61f633923144"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the function takes the y-values in the training data as the input and makes the bar chart. \n",
    "def plot_bar_chart_score(y):\n",
    "    fix, ax = plt.subplots()\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    plt.bar(unique, counts)\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Number of wines')\n",
    "    plt.title('Distribution of scores of wines')\n",
    "    plt.show()\n",
    "\n",
    "plot_bar_chart_score(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGuNg0KbWN0z"
   },
   "source": [
    "Our goal is to train a model that can take wine records in X and predict the quality of the wines.\n",
    "We split the data into training data and test data. \n",
    "In practice, we should sample randomly 80% of the data as training data and the rest as the test data. \n",
    "Though, in this practical, let's all use the same split: the **first** 80% of the data for training and the remaining 20% for testing. This makes sure we can compare your results to ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1** \n",
    "The first task is to write a function that can split the dataset into training and testing data according to the parameter `split_coeff`. When `split_coeff` is set to 0.8, it means the function sets the first 80% of the data as the training data and the remaining 20% as the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1416,
     "status": "ok",
     "timestamp": 1596436129239,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "6ZqbBa8bWNYg",
    "outputId": "da274c4e-c3ed-4ac0-8442-27befcf26f4c"
   },
   "outputs": [],
   "source": [
    "# The function splits the dataset into training data and testing data.\n",
    "# The parameter split_coeff is a percentage value such that\n",
    "# the first split_coeff of the dataset become the training data\n",
    "# and the remaining data become the test data\n",
    "def split_data(X, y, split_coeff=0.8):\n",
    "    # The code below is just for compilation. \n",
    "    # You need to delete it and write your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    \n",
    "    N, _ = X.shape # get the number of records (rows)\n",
    "    train_size = int(split_coeff * N) # use the first split_coeff of the data as the training data\n",
    "    X_train = X[:train_size] # the first training_size records\n",
    "    y_train = y[:train_size]\n",
    "    X_test = X[train_size:] # the last test_size records\n",
    "    y_test = y[train_size:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #X_train,X_test = np.split(X,[int(split_coeff * len(X))])\n",
    "    #y_train,y_test = np.split(y,[int(split_coeff * len(y))])\n",
    "    \n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function should return the expected output as shown in the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (3918, 11)\n",
      "Shape of y_train: (3918,)\n",
      "Shape of X_test: (980, 11)\n",
      "Shape of y_test: (980,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_data(X, y, 0.8) # split the data with split_coeff=0.8\n",
    "\n",
    "# check the size of the splitted dataset\n",
    "print(\"Shape of X_train:\", X_train.shape) # expected output (3918, 11)\n",
    "print(\"Shape of y_train:\", y_train.shape) # expected output (3918,)\n",
    "print(\"Shape of X_test:\", X_test.shape) # expected output (980, 11)\n",
    "print(\"Shape of y_test:\", y_test.shape) # expected output (980,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GxjlElni2FcH"
   },
   "source": [
    "### **Task 2** \n",
    "\n",
    "As a first step, we construct a trivial model that returns the mean of the y-values in the training data for any wine record in X. We use this trivial model as a baseline. The linear regression models we build later should perform better than this trivial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1402,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "-V3xFYexX1lt",
    "outputId": "5e57738e-87d5-408c-f1bf-9df66a175f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of the y-values in the training data is 5.878764675855028\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "# Task 2: implement the trivial predictor\n",
    "# The function computes the average value of y on the training label values\n",
    "def compute_average(y_train):\n",
    "    # The code below is just for compilation. \n",
    "    # You need to delete it and write your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hint: return the mean of y_train\n",
    "    average=np.mean(y_train)\n",
    "    return average\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "y_train_avg = compute_average(y_train)\n",
    "print(\"The average of the y-values in the training data is {}\".format(y_train_avg)) \n",
    "\n",
    "# The trivial predictor returns the average value.\n",
    "def trivial_predictor(X_test, y_train_avg):\n",
    "  return y_train_avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x531Q_SxXV14"
   },
   "source": [
    "### **Task 3**\n",
    "We next evaluate the trivial predictor on the training data and test data. \n",
    "We use mean squared error (MSE) to measure the performance of the predictor.\n",
    "The task is to implement a function that reports the mean squared error of the given predictor on the given data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1394,
     "status": "ok",
     "timestamp": 1596436129240,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "mV8l6Ci9YlgL",
    "outputId": "f57858dc-d0fc-40fe-dbf7-c652d2f8fddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trivial Predictor\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MSE (Training) = 0.7768\n",
      "MSE (Testing)  = 0.8139\n"
     ]
    }
   ],
   "source": [
    "# We next test our trivial predictor on the training data and test data. \n",
    "# Implement a function that can report the mean squared error \n",
    "# of a predictor on the given data\n",
    "# Input: data and predictor\n",
    "# Output: mean squared error of the predictor on the given data\n",
    "def test_predictor(X, y, predictor: callable=None):\n",
    "    # Apply the predictor to each row of the matrix X to get the predictions\n",
    "    y_predicted = np.apply_along_axis(predictor, 1, X)\n",
    "\n",
    "    # TODO: compute the mean squared error of y\n",
    "    # The code below is just for compilation. \n",
    "    # You need to delete it and write your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    mse=np.square(np.subtract(y,y_predicted)).mean()\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# use the function test_predictor to test the trivial predictor\n",
    "# we use the lambda function here to pass the function trivial predictor to the function test_predictor.\n",
    "mse_trivial_predictor_train = test_predictor(X_train, y_train, lambda x: trivial_predictor(x, y_train_avg))\n",
    "mse_trivial_predictor_test = test_predictor(X_test, y_test, lambda x: trivial_predictor(x, y_train_avg))\n",
    "\n",
    "# Report the result\n",
    "print('Trivial Predictor')\n",
    "print('--------------------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_trivial_predictor_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_trivial_predictor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "geiyM1Nea0az"
   },
   "source": [
    "## Train the Linear Model Using the Least Squares Method\n",
    "\n",
    "Let's next train a linear regression model on the training data. \n",
    "We use the closed form solution of the least squares estimate to compute the parameters of the linear regression model. \n",
    "This model should perform better than the trivial predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WSEwFGp_bqAI"
   },
   "source": [
    "### **Task 4**\n",
    "Before training the model, we need to standardize the data, i.e., transform the data so that every feature has mean 0 and variance 1. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Standard_score\n",
    "\n",
    "We first standardize the training data, and then apply the same transformation to the test data. That is, standardize the test data using the means and the standard deviations of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1387,
     "status": "ok",
     "timestamp": 1596436129241,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "trjwkcgybhDH",
    "outputId": "d87a4635-354f-47e2-947a-e843f027e4cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_std: (3918, 11)\n",
      "Mean: [6.85427514e+00 2.78390761e-01 3.34892802e-01 6.42623788e+00\n",
      " 4.58213374e-02 3.53263144e+01 1.38513272e+02 9.94040729e-01\n",
      " 3.18647524e+00 4.89055641e-01 1.05115799e+01]\n",
      "Standard deviation: [8.39100902e-01 9.95630176e-02 1.24249975e-01 5.06377532e+00\n",
      " 2.16660282e-02 1.71004677e+01 4.23956179e+01 2.97972269e-03\n",
      " 1.49949475e-01 1.12992053e-01 1.22536544e+00]\n"
     ]
    }
   ],
   "source": [
    "# The task is to implement a function that can standardize the data and returns the mean and std of the data.\n",
    "# Input: training data\n",
    "# Output: standardize training data, standard deviations and means\n",
    "def standardize_data(X):\n",
    "    # TODO: compute the means and standard deviations of the data, and standardize the data\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    \n",
    "    normX=X.copy()\n",
    "    mean=np.mean(normX,axis=0)\n",
    "    std=np.std(normX,axis=0)\n",
    "    X_std=(normX-mean)/std\n",
    "    \n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "    return X_std, mean, std\n",
    "\n",
    "# Standardize the training data and store the means and the stds \n",
    "X_train_std, X_train_mean, X_train_std_div = standardize_data(X_train)\n",
    "print(\"X_train_std:\", X_train_std.shape)\n",
    "print(\"Mean:\", X_train_mean)\n",
    "print(\"Standard deviation:\", X_train_std_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1380,
     "status": "ok",
     "timestamp": 1596436129242,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "RjzbA5JpM759",
    "outputId": "ff594788-2fdd-419c-98fa-beac6a53cfc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(980, 11)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Standardize the test data using the means and standrad deviations of the training data\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "X_test_std = (X_test.copy()-X_train_mean)/X_train_std_div\n",
    "print(X_test_std.shape)\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "################################################### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vT4_Sl42bxmD"
   },
   "source": [
    "### **Task 5**\n",
    "Let's now train the linear model using the least-squares method. \n",
    "We need to add the bias term to the matrix X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1374,
     "status": "ok",
     "timestamp": 1596436129242,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "A4JtLr6pdJV7",
    "outputId": "dfd57312-284f-4ce9-820b-4fdbdfbec8c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: (12,)\n"
     ]
    }
   ],
   "source": [
    "# The task is to implement the function that adds a column of ones to the first column of X\n",
    "def expand_with_ones(X):\n",
    "    # TODO: add a column of ones to the front of the input matrix\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    N = X.shape[0]\n",
    "    X_out=np.c_[ np.ones(N),X  ]  \n",
    "    return X_out\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "# Train the linear model using the least-squares method\n",
    "# The task is to implement the function that computes the parameters\n",
    "def least_squares_compute_parameters(X_input, y):\n",
    "    # add the bias column to the data\n",
    "    X = expand_with_ones(X_input)\n",
    "\n",
    "    # TODO: compute the parameters based on the expanded X and y using the least-squares method\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    w=np.linalg.inv(X.transpose()@X)@X.transpose()@y\n",
    "    #w = np.zeros(X.shape[1])\n",
    "    return w\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "# Apply the function to train the linear model\n",
    "w = least_squares_compute_parameters(X_train_std, y_train) \n",
    "print(\"w:\", w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lasj_1PpeZib"
   },
   "source": [
    "After computing the parameters,\n",
    "we can build the linear model predictor.\n",
    "The predictor takes as input the computed parameters and the data, and predicts the labels for the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lb-hNagxc3Wj"
   },
   "outputs": [],
   "source": [
    "# Implement the linear model predictor\n",
    "# Input: test data and parameters\n",
    "# Output: predicted values\n",
    "def linear_model_predictor(X, w):\n",
    "    # TODO: predict the labels for the input data\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    y_predicted=X@w\n",
    "    \n",
    "    return y_predicted\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFOYpwbufz7J"
   },
   "source": [
    "Evaluate our linear regression model predictor on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1363,
     "status": "ok",
     "timestamp": 1596436129243,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "LuHHmn2RB55j",
    "outputId": "b6cb4556-2618-419a-a082-214f2e6ecb5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error is 0.560729204228347\n"
     ]
    }
   ],
   "source": [
    "# use the function test_predictor to evaluate the linear model predictor\n",
    "mse_linear_model_predictor = test_predictor(expand_with_ones(X_test_std), y_test, lambda x: linear_model_predictor(x, w))\n",
    "print(\"Mean squared error is {}\".format(mse_linear_model_predictor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqj4HKAihF7Q"
   },
   "source": [
    "## Learning Curves\n",
    "\n",
    "Let us check if the linear model is overfitting or underfitting. Since the dataset is somewhat large and there are only 11 features, the model shouldn't be overfitting. \n",
    "To check it, we use the learning curves: we plot how the performance of the model changes when it is trained with increasingly more data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNf11kurCgKF"
   },
   "source": [
    "### **Task 6** \n",
    "\n",
    "Let's first implement a function that comprises what we have implemented above. \n",
    "The function takes as inputs the data and the split coefficient, and then\n",
    "1. standardizes the data,\n",
    "2. trains the linear model (compute the parameters), and\n",
    "3. reports the MSE of the linear model predictor on both the training and the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1355,
     "status": "ok",
     "timestamp": 1596436129244,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "UcGRQBrEb106",
    "outputId": "179c5ec0-ee87-4c4b-a02b-d97d55862e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model\n",
      "-----------------------\n",
      "\n",
      "MSE (Training) = 0.5640\n",
      "MSE (Testing)  = 0.5607\n"
     ]
    }
   ],
   "source": [
    "# Input: training data and test data\n",
    "# Output: mse of the linear model predictor on both the training and test data\n",
    "def train_and_test(X_train, y_train, X_test, y_test):\n",
    "    # TODO: implement the function \n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: use the functions you have implemented\n",
    "    \n",
    "    #Standartize data\n",
    "    X_train_std, X_train_mean, X_train_std_div = standardize_data(X_train)\n",
    "    X_test_std=(X_test.copy()-X_train_mean)/X_train_std_div\n",
    "    \n",
    "    #Train linear model&Predict\n",
    "    w=least_squares_compute_parameters(X_train_std, y_train)\n",
    "\n",
    "    \n",
    "    #Report the MSE\n",
    "    mse_train =test_predictor(X_train_std, y_train, lambda x: linear_model_predictor(expand_with_ones(X_train_std), w))\n",
    "    mse_test =test_predictor(X_test_std, y_test, lambda x: linear_model_predictor(expand_with_ones(X_test_std), w))\n",
    "    \n",
    "    return mse_train, mse_test\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "mse_train, mse_test = train_and_test(X_train, y_train, X_test, y_test)\n",
    "print('Linear Model')\n",
    "print('-----------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NTJw_BrzhRwi"
   },
   "source": [
    "### **Task 7**\n",
    "\n",
    "We are now ready to plot the learning curves. \n",
    "\n",
    "We train a list of models on the increasingly more training data ([20, 40, ..., 600] data records).\n",
    "For each of such model, we compute the MSEs of the model on both the training data and the test data, and store the MSEs in the lists `mse_train_v` and `mse_test`, respectively. \n",
    "The code provided below will then plot the learning curves.\n",
    "\n",
    "Your plot should show the two curves (roughly) meet at round 150 training data records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1747,
     "status": "ok",
     "timestamp": 1596436129644,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "jDsdh4T3hcIU",
    "outputId": "621c4890-1c55-4e9b-f28f-33d60907d8b9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse_train_v = []\n",
    "mse_test_v = []\n",
    "\n",
    "TRAINING_SIZE_MAX = 601\n",
    "TRAINING_SIZE_MIN = 20\n",
    "\n",
    "# compute the MSE over data with sizes from TRAINING_SIZE_MIN to TRAINING_SIZE_MAX with increasing step 20\n",
    "for train_size in range(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20):\n",
    "    # TODO: \n",
    "    #   1. use the first train_size data records from the X_train and y_train as the training data\n",
    "    #   2. train and compute the MSE on both training and test data using the train_and_test function\n",
    "    #   3. add the computed MSE to the lists mse_train_v and mse_test_v\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "   \n",
    "    mse_train, mse_test = train_and_test(X_train[:train_size,], y_train[:train_size], X_test, y_test)\n",
    "   \n",
    "    \n",
    "    mse_train_v.append(mse_train)\n",
    "    mse_test_v.append(mse_test)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "    \n",
    "\n",
    "# The below code generates the learning curves plot\n",
    "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_train_v, 'r--', label=\"Training Error\")\n",
    "plt.plot(np.arange(TRAINING_SIZE_MIN, TRAINING_SIZE_MAX, 20), mse_test_v, 'b-', label=\"Test Error\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djpsaTu_kK3T"
   },
   "source": [
    "## Polynomial Basis Expansion and Regularisation\n",
    "\n",
    "In this part, we will improve the linear regression model by basis expansion and regularization: \n",
    "1. apply the degree 2 basis expansion to the data, \n",
    "2. build the Ridge and Lasso models and\n",
    "3. perform hyperparameter optimization using the validation data. \n",
    "\n",
    "For the hyperparameter optimization, you should set the last 20% of the training data for the purpose of validation and try lambda values in the range [10^-4, 10^-3, 10^-2, 10^-1, 1, 10, 100]. \n",
    "\n",
    "We will use the scikit-learn library. We have imported the necessary functions for you. You can import other scikit-learn functions if you think they are useful. The documentation is available here: http://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TM0nkNbkhfM"
   },
   "outputs": [],
   "source": [
    "# import the preprocessing libs for standarization and basis expansion\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures \n",
    "\n",
    "# Ridge and Lasso linear model\n",
    "from sklearn.linear_model import Ridge, Lasso "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCwBPuOXlRF7"
   },
   "source": [
    "### **Task 8**\n",
    "Let's implement the function for expanding the basis of the dataset. \n",
    "\n",
    "Hints: use `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50azFolql1qA"
   },
   "outputs": [],
   "source": [
    "def expand_basis(X, degree):\n",
    "    # TODO: expand the basis of X for the input degree\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: use the function PolynomialFeatures\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    return poly.fit_transform(X)\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jwkPevimQri"
   },
   "source": [
    "### **Task 9**\n",
    "We need to prepare the data.\n",
    "We first expand and standardize the data,\n",
    "and then split the training data to training data and validation data.\n",
    "We use the last 20% of the training data as the validation data.\n",
    "\n",
    "Hints: use `StandardScaler` and `std_scaler` to standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQCq4G9YmW7w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_n: (3134, 78)\n",
      "Shape of y_train_n: (3134,)\n",
      "Shape of X_train_v: (784, 78)\n",
      "Shape of y_train_v: (784,)\n",
      "Shape of X_test: (980, 78)\n",
      "Shape of y_test: (980,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(X, y, degree):\n",
    "    # TODO: the training, test and validation data using the expanded dataset.\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    # Hints: follow the steps \n",
    "    std_scaler=StandardScaler()\n",
    "    \n",
    "    # 1. split the data into training and test data\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, 0.8)\n",
    "    \n",
    "    \n",
    "    # 2. standardize the training data and test data\n",
    "    X_train = std_scaler.fit_transform(X_train)\n",
    "    X_test = std_scaler.transform(X_test)\n",
    "    \n",
    "    \n",
    "    # 3. basis expansion of the training data and test data\n",
    "    X_train = expand_basis(X_train, degree)\n",
    "    X_test = expand_basis(X_test, degree)\n",
    "    \n",
    "    \n",
    "    # 4. split the expanded training data into training data (X_train_n, y_train_n) and validation data (X_train_v, y_train_v)\n",
    "    X_train_n, y_train_n, X_train_v, y_train_v = split_data(X_train, y_train, 0.8)\n",
    "    \n",
    "    \n",
    "    # 5. standardize the training data and validation data\n",
    "    X_train_n = std_scaler.fit_transform(X_train_n)\n",
    "    X_train_v = std_scaler.transform(X_train_v)\n",
    "   \n",
    "\n",
    "    return X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test\n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "X_train, y_train, X_train_n, y_train_n, X_train_v, y_train_v, X_test, y_test = prepare_data(X, y, 2) # here we expand the dataset with degree 2\n",
    "\n",
    "# check the size of the splitted dataset\n",
    "print(\"Shape of X_train_n:\", X_train_n.shape) # expected output (3134, 78)\n",
    "print(\"Shape of y_train_n:\", y_train_n.shape) # expected output (3134,)\n",
    "print(\"Shape of X_train_v:\", X_train_v.shape) # expected output (784, 78)\n",
    "print(\"Shape of y_train_v:\", y_train_v.shape) # expected output (784,)\n",
    "print(\"Shape of X_test:\", X_test.shape) # expected output (980, 78)\n",
    "print(\"Shape of y_test:\", y_test.shape) # expected output (980,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3BxxtM3nghU"
   },
   "source": [
    "### **Task 10**\n",
    "We have prepared the training data and the validation data. \n",
    "We can now use the validation data to select the optimal hyperparameters for the Ridge and Lasso models. \n",
    "We use the Ridge and Lasso models from scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "We train Ridge or Lasso models with different lambda values and check their performance on the validation data, and select the lambda values that yield the best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3266,
     "status": "ok",
     "timestamp": 1596436131187,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "SvXcAGW1oHq1",
    "outputId": "25a38d1f-013f-4b0a-9cbb-3f08b68c0371",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 267.96844645467957, tolerance: 0.24240462667517543\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9041018199147857, tolerance: 0.24240462667517543\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge lambda: 1\n",
      "Lasso lambda: 0.001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The function takes the training and validation data as inputs, and \n",
    "# returns the lambda value that results the minimal mse\n",
    "# We use is_ridge to indicate which the model is considered.\n",
    "# is_ridge = True indicates Ridge while is_ridge = False indicates Lasso\n",
    "def choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, is_ridge: bool):\n",
    "    mse_arr = []\n",
    "    lam_arr = []\n",
    "\n",
    "    # Try lambda values from 10^-4 to 10^2. \n",
    "    # Record the mse and the lambda values in mse_arr and lam_arr\n",
    "    # The code below is just for compilation. \n",
    "    # You need to replace it by your own code.\n",
    "    ###################################################\n",
    "    ##### YOUR CODE STARTS HERE #######################\n",
    "    ###################################################\n",
    "    for pow_lam in range(-4, 3):\n",
    "        lam = 10 ** pow_lam\n",
    "        mse=0\n",
    "        if is_ridge == True:\n",
    "            rr = Ridge(alpha=lam)\n",
    "            rr.fit(X_train_n, y_train_n) \n",
    "            pred_test= rr.predict(X_train_v)\n",
    "        else:\n",
    "            lasso = Lasso(alpha=lam)\n",
    "            lasso.fit(X_train_n, y_train_n) \n",
    "            pred_test= lasso.predict(X_train_v)\n",
    "        \n",
    "        \n",
    "        mse = np.mean((y_train_v-pred_test)**2) # compute the mse for this lam\n",
    "        mse_arr.append(mse) \n",
    "        lam_arr.append(lam) \n",
    "    ###################################################\n",
    "    ##### YOUR CODE ENDS HERE #########################\n",
    "    ###################################################\n",
    "\n",
    "\n",
    "    # get the index of the lambda value that has the minimal use\n",
    "    lambda_idx_min = np.argmin(np.array(mse_arr))\n",
    "\n",
    "    # plot of the lambda values and their mse\n",
    "    plt.figure()\n",
    "    plt.semilogx(lam_arr, mse_arr)\n",
    "\n",
    "    # return the optimal lambda value\n",
    "    return lam_arr[lambda_idx_min]\n",
    "\n",
    "# call the function to choose the lambda for Ridge and Lasso\n",
    "lam_ridge = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, True)\n",
    "lam_lasso = choose_hyper_param(X_train_n, y_train_n, X_train_v, y_train_v, False)\n",
    "\n",
    "print(\"Ridge lambda:\", lam_ridge)\n",
    "print(\"Lasso lambda:\", lam_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAuX0uU5k9qD"
   },
   "source": [
    "### **Task 11**:\n",
    "Once we get the optimal lambdas for Ridge and Lasso, we train these models using these lambdas on the full training data, and then report their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3259,
     "status": "ok",
     "timestamp": 1596436131187,
     "user": {
      "displayName": "Haozhe Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhewCb1FImnjURCLugyfalL8wVXJomnuoEHUckN=s64",
      "userId": "15943369882491692800"
     },
     "user_tz": -480
    },
    "id": "VmwHESkg77zK",
    "outputId": "9bb9c1cf-1649-40e6-9162-2244525d9446",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ridge Regression with using degree 2 polynomial expansion and lambda = 1.0000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MSE (Training) = 0.4952\n",
      "MSE (Testing)  = 0.5125\n",
      "\n",
      "\n",
      "For Lasso with using degree 2 polynomial expansion and lambda = 0.0010\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "MSE (Training) = 0.4962\n",
      "MSE (Testing)  = 0.5098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6056512715367717, tolerance: 0.304341322103114\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# TODO: train the Ridge and Lasso models using the optimal parameters, and\n",
    "#       report their MSE\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "# Hints: train these models on the full training data\n",
    "\n",
    "\n",
    "#Ridge\n",
    "ridge = Ridge(alpha=lam_ridge)    \n",
    "ridge.fit(X_train, y_train) \n",
    "pred_train_ridge= ridge.predict(X_train)\n",
    "mse_ridge_train = np.mean((y_train-pred_train_ridge)**2)\n",
    "\n",
    "pred_test_ridge= ridge.predict(X_test)\n",
    "mse_ridge_test = np.mean((y_test-pred_test_ridge)**2)\n",
    "\n",
    "\n",
    "\n",
    "#Lasso\n",
    "lasso = Lasso(alpha=lam_lasso)\n",
    "lasso.fit(X_train, y_train) \n",
    "pred_train_lasso= lasso.predict(X_train)\n",
    "mse_lasso_train = np.mean((y_train-pred_train_lasso)**2)\n",
    "\n",
    "pred_test_lasso= lasso.predict(X_test)\n",
    "mse_lasso_test = np.mean((y_test-pred_test_lasso)**2)\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################\n",
    "\n",
    "# Report the result\n",
    "print('For Ridge Regression with using degree %d polynomial expansion and lambda = %.4f' % (2, lam_ridge))\n",
    "print('--------------------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_ridge_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_ridge_test)\n",
    "\n",
    "print('\\n\\nFor Lasso with using degree %d polynomial expansion and lambda = %.4f' % (2, lam_lasso))\n",
    "print('---------------------------------------------------------------------\\n')\n",
    "print('MSE (Training) = %.4f' % mse_lasso_train)\n",
    "print('MSE (Testing)  = %.4f' % mse_lasso_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Os9tKKLd8gMU"
   },
   "source": [
    "## Optional: Try Larger Degrees using K-fold Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfqRAlv1PBXi"
   },
   "source": [
    "### **Task 12**\n",
    "This is an optional task, which worths 5 bonus points.\n",
    "\n",
    "The task is to try basis expansions with higher degrees (up to degree 4) and find the degree that results the best performance. \n",
    "Instead of always using the same validation data, we use k-fold cross-validation to find the optimal hyperparameters. \n",
    "\n",
    "Your task is to report the optimal hyperparameters (the basis expansion degree and the lambdas) and the MSEs of the Ridge and Lasso when using these optimal hyperparameters. \n",
    "\n",
    "Hints: Use `KFold` to do this automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpwY7UtQ8l-0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65.76339820180715, tolerance: 0.24286818761965553\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.50308775430517, tolerance: 0.24814170389278872\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 45.02572786038104, tolerance: 0.2422490427568606\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 106.08495806600502, tolerance: 0.24151814992025522\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 262.4746620950373, tolerance: 0.24253282296650705\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.657636651947314, tolerance: 0.2422490427568606\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8636415878356729, tolerance: 0.24253282296650705\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 346.2195301664725, tolerance: 0.24286818761965553\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 301.20858003173754, tolerance: 0.24814170389278872\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 435.7142963873415, tolerance: 0.2422490427568606\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 447.91159427767775, tolerance: 0.24151814992025522\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 440.4598740624939, tolerance: 0.24253282296650705\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.232135769617344, tolerance: 0.24286818761965553\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.3774281998219067, tolerance: 0.24814170389278872\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15.065055029407631, tolerance: 0.2422490427568606\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.91256133858724, tolerance: 0.24151814992025522\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.602372530661796, tolerance: 0.24253282296650705\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 327.08147233781915, tolerance: 0.24286818761965553\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 340.8222628498589, tolerance: 0.24814170389278872\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323.0997211653082, tolerance: 0.2422490427568606\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 255.7213511075551, tolerance: 0.24151814992025522\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 340.0518499270122, tolerance: 0.24253282296650705\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 84.03857170113974, tolerance: 0.24286818761965553\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.378735767527473, tolerance: 0.24814170389278872\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 108.4250962039066, tolerance: 0.2422490427568606\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.088048635336236, tolerance: 0.24151814992025522\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 113.70184141476204, tolerance: 0.24253282296650705\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyperparameter for Ridge Regression:\n",
      "Degree 2 polynomial expansion\n",
      "Lambda = 10.0000\n",
      "Min ridge averageMSE  = 0.5389\n",
      "\n",
      "\n",
      "Optimal hyperparameter for Lasso Regression:\n",
      "Degree 2 polynomial expansion\n",
      "Lambda = 0.0010\n",
      "Min lasso average MSE  = 0.5378\n",
      "\n",
      "---------------------------------------------\n",
      "MSE when the optimal hyperparameters are applied:\n",
      "\n",
      "For Ridge Regression with using degree 2 polynomial expansion and lambda = 10.0000\n",
      "Ridge MSE: 0.5117\n",
      "\n",
      "For Lasso with using degree 2 polynomial expansion and lambda = 0.0010\n",
      "Lasso MSE: 0.5098\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.6056512715367717, tolerance: 0.304341322103114\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = False)\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "#standardize\n",
    "std_scaler.fit(X_train)\n",
    "X_train_std=std_scaler.transform(X_train)\n",
    "X_test_std=std_scaler.transform(X_test)\n",
    "   \n",
    "kf = KFold()\n",
    "\n",
    "min_mse_ridge = float('inf')\n",
    "min_mse_lasso = float('inf')\n",
    "\n",
    "optimal_degree_ridge = 0\n",
    "optimal_degree_lasso = 0\n",
    "\n",
    "optimal_lam_ridge = 0\n",
    "optimal_lam_lasso = 0\n",
    "\n",
    "\n",
    "for expand_degree in range(1,5):\n",
    "    #expand basis\n",
    "    X_train_exp = expand_basis(X_train_std, expand_degree)\n",
    "    X_test_exp = expand_basis(X_test_std, expand_degree)\n",
    "\n",
    "    \n",
    "    for pow_lam in range(-4, 3):\n",
    "        lam = 10 ** pow_lam\n",
    "        ridge = Ridge(alpha = lam)\n",
    "        lasso = Lasso(alpha = lam)\n",
    "        \n",
    "        mse_ridge_arr = []\n",
    "        mse_lasso_arr = []\n",
    "        \n",
    "        for train_index, validation_index in kf.split(X_train_exp):\n",
    "            \n",
    "            X_train_n, X_train_v, y_train_n, y_train_v = X_train_exp[train_index], X_train_exp[validation_index], y_train[train_index], y_train[validation_index]\n",
    "\n",
    "            #standardize training data and validation data\n",
    "            X_train_n_std=std_scaler.fit_transform(X_train_n)\n",
    "            X_train_v_std=std_scaler.transform(X_train_v)\n",
    "            \n",
    "            #ridge \n",
    "            ridge.fit(X_train_n_std, y_train_n)\n",
    "            y_predicted_ridge = ridge.predict(X_train_v_std)\n",
    "            mse_ridge_arr.append(np.mean((y_train_v-y_predicted_ridge)**2))\n",
    "            \n",
    "            #lasso \n",
    "            lasso.fit(X_train_n_std, y_train_n)\n",
    "            y_predicted_lasso = lasso.predict(X_train_v_std)\n",
    "            mse_lasso_arr.append(np.mean((y_train_v-y_predicted_lasso)**2))\n",
    "        \n",
    "        # average mse of all folds for each lambda\n",
    "        avg_mse_ridge = np.mean(mse_ridge_arr)\n",
    "        if avg_mse_ridge < min_mse_ridge:\n",
    "            min_mse_ridge = avg_mse_ridge\n",
    "            optimal_degree_ridge = expand_degree\n",
    "            optimal_lam_ridge = lam\n",
    "            \n",
    "        avg_mse_lasso = np.mean(mse_lasso_arr)\n",
    "        if avg_mse_lasso < min_mse_lasso:\n",
    "            min_mse_lasso = avg_mse_lasso\n",
    "            optimal_degree_lasso = expand_degree\n",
    "            optimal_lam_lasso = lam\n",
    "\n",
    "\n",
    "# Apply optimal parameters on test dataset\n",
    "\n",
    "# Ridge regularization\n",
    "X_train_exp = expand_basis(X_train_std, optimal_degree_ridge)\n",
    "X_test_exp = expand_basis(X_test_std, optimal_degree_ridge)\n",
    "\n",
    "#X_train_exp=std_scaler.fit_transform(X_train_exp)\n",
    "#X_test_exp=std_scaler.transform(X_test_exp)\n",
    "\n",
    "ridge = Ridge(alpha = optimal_lam_ridge)\n",
    "ridge.fit(X_train_exp, y_train)\n",
    "y_predicted_ridge = ridge.predict(X_test_exp)\n",
    "ridge_mse = np.mean((y_test-y_predicted_ridge)**2)\n",
    "\n",
    "\n",
    "# Lasso regularization\n",
    "X_train_exp = expand_basis(X_train_std, optimal_degree_lasso)\n",
    "X_test_exp = expand_basis(X_test_std, optimal_degree_lasso)\n",
    "\n",
    "#X_train_exp=std_scaler.fit_transform(X_train_exp)\n",
    "#X_test_exp=std_scaler.transform(X_test_exp)\n",
    "\n",
    "lasso = Lasso(alpha = optimal_lam_lasso)\n",
    "lasso.fit(X_train_exp, y_train)\n",
    "y_predicted_lasso = lasso.predict(X_test_exp)\n",
    "lasso_mse = np.mean((y_test-y_predicted_lasso)**2)\n",
    "\n",
    "\n",
    "#Report results\n",
    "\n",
    "print('Optimal hyperparameter for Ridge Regression:')\n",
    "print('Degree %d polynomial expansion' % optimal_degree_ridge)\n",
    "print('Lambda = %.4f' % optimal_lam_ridge)\n",
    "print('Min ridge averageMSE  = %.4f' % min_mse_ridge)\n",
    "\n",
    "print('\\n\\nOptimal hyperparameter for Lasso Regression:')\n",
    "print('Degree %d polynomial expansion' % optimal_degree_lasso)\n",
    "print('Lambda = %.4f' % optimal_lam_lasso)\n",
    "print('Min lasso average MSE  = %.4f\\n' % min_mse_lasso)\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "print(\"MSE when the optimal hyperparameters are applied:\")\n",
    "print('\\nFor Ridge Regression with using degree %d polynomial expansion and lambda = %.4f' % (optimal_degree_ridge, optimal_lam_ridge))\n",
    "print(\"Ridge MSE: %.4f\" % ridge_mse)\n",
    "print('\\nFor Lasso with using degree %d polynomial expansion and lambda = %.4f' % (optimal_degree_lasso, optimal_lam_lasso))\n",
    "print(\"Lasso MSE: %.4f\\n\" % lasso_mse)\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyperparameter for Ridge Regression:\n",
      "Degree 1 polynomial expansion\n",
      "Lambda = 0.0010\n",
      "MSE  = 0.5696\n",
      "\n",
      "\n",
      "Optimal hyperparameter for Lasso Regression:\n",
      "Degree 1 polynomial expansion\n",
      "Lambda = 0.0001\n",
      "MSE  = 0.5745\n",
      "\n",
      "MSE when the optimal hyperparameters are applied:\n",
      "\n",
      "For Ridge Regression with using degree 1 polynomial expansion and lambda = 0.0010\n",
      "Ridge MSE: 0.5607\n",
      "\n",
      "\n",
      "For Lasso with using degree 1 polynomial expansion and lambda = 0.0001\n",
      "Lasso MSE: 0.5608\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE STARTS HERE #######################\n",
    "###################################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler=StandardScaler()\n",
    "\n",
    "mse_lasso_arr = []\n",
    "mse_ridge_arr = []\n",
    "lam_lasso_arr = []\n",
    "lam_ridge_arr = []\n",
    "deg_lasso_arr = []\n",
    "deg_ridge_arr = []\n",
    "\n",
    "\n",
    "#Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, shuffle = False)\n",
    "\n",
    "#Standartize\n",
    "X_train = std_scaler.fit_transform(X_train)\n",
    "X_test = std_scaler.transform(X_test)\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "#Go over all possible degrees\n",
    "for degree in range(1,5):\n",
    "    \n",
    "    #expand\n",
    "    X_train_exp = expand_basis(X_train, expand_degree)\n",
    "    X_test_exp = expand_basis(X_test, expand_degree)\n",
    "\n",
    "    for pow_lam in range(-4, 5):\n",
    "        \n",
    "        lam = 10 ** pow_lam\n",
    "        \n",
    "        ridge = Ridge(alpha = lam)\n",
    "        lasso = Lasso(alpha = lam)\n",
    "        \n",
    "        mse_ridge_forlam = []\n",
    "        mse_lasso_forlam = []\n",
    "        \n",
    "        for train_index, val_index in kf.split(X_train):\n",
    "\n",
    "            X_trainCV, X_val, y_trainCV, y_val = X[train_index], X[val_index], y[train_index], y[val_index]\n",
    "            \n",
    "            #Standartize\n",
    "            #X_trainCV = std_scaler.fit_transform(X_trainCV)\n",
    "            #X_val = std_scaler.transform(X_val)\n",
    "            \n",
    "            #Expand the basis of the training data and test data\n",
    "            #X_trainCV=poly.fit_transform(X_trainCV)\n",
    "            #X_val=poly.fit_transform(X_val)\n",
    "            \n",
    "            #X_trainCV = std_scaler.fit_transform(X_trainCV)\n",
    "            #X_val = std_scaler.fit_transform(X_val)\n",
    "        \n",
    "            #Ridge  \n",
    "            ridge.fit(X_trainCV, y_trainCV) \n",
    "            pred_train_ridge= ridge.predict(X_val)\n",
    "            mse_ridge_forlam.append(np.mean((y_val-pred_train_ridge)**2))\n",
    "\n",
    "            #Lasso\n",
    "            lasso.fit(X_trainCV, y_trainCV) \n",
    "            pred_train_lasso= lasso.predict(X_val)\n",
    "            mse_lasso_forlam.append(np.mean((y_val-pred_train_lasso)**2))\n",
    "\n",
    " \n",
    "        # average mse of all folds for each lambda\n",
    "        mse_rr_avg_lam = np.mean(mse_ridge_forlam) \n",
    "        mse_lasso_avg_lam = np.mean(mse_lasso_forlam)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # for Ridge\n",
    "        mse_ridge_arr.append(mse_rr_avg_lam)      \n",
    "        lam_ridge_arr.append(lam)                        \n",
    "        deg_ridge_arr.append(degree)                         \n",
    "        # for Lasso\n",
    "        mse_lasso_arr.append(mse_lasso_avg_lam)     \n",
    "        lam_lasso_arr.append(lam)                        \n",
    "        deg_lasso_arr.append(degree)  \n",
    "        \n",
    "        \n",
    "# Get optimal hyperparameters\n",
    "#Ridge\n",
    "index_r = np.argmin(mse_ridge_arr) \n",
    "mse_ridge_value = mse_ridge_arr[index_r]\n",
    "lam_ridge_value = lam_ridge_arr[index_r]\n",
    "d_ridge_value = deg_ridge_arr[index_r]\n",
    "#lasso\n",
    "index_l = np.argmin(mse_lasso_arr) \n",
    "mse_lasso_value = mse_lasso_arr[index_l]\n",
    "lam_lasso_value = lam_lasso_arr[index_l]\n",
    "d_lasso_value = deg_lasso_arr[index_l]\n",
    "\n",
    "\n",
    "\n",
    "# Apply optimal parameters on test dataset\n",
    "\n",
    "# Ridge regularization\n",
    "poly = PolynomialFeatures(d_ridge_value)\n",
    "X_train_rr = poly.fit_transform(X_train)\n",
    "X_test_rr = poly.fit_transform(X_test)\n",
    "clf_rr = Ridge(alpha = lam_ridge_value)\n",
    "clf_rr.fit(X_train_rr, y_train)\n",
    "y_pred_rr = clf_rr.predict(X_test_rr)\n",
    "mse_ridge =  np.mean((y_test-y_pred_rr)**2)\n",
    "\n",
    "\n",
    "\n",
    "# Lasso regularization\n",
    "poly = PolynomialFeatures(d_lasso_value)\n",
    "X_train_lasso = poly.fit_transform(X_train)\n",
    "X_test_lasso = poly.fit_transform(X_test)\n",
    "clf_lasso = Lasso(alpha = lam_lasso_value)\n",
    "clf_lasso.fit(X_train_lasso, y_train)\n",
    "y_pred_lasso = clf_lasso.predict(X_test_lasso)\n",
    "mse_lasso =  np.mean((y_test-y_pred_lasso)**2)\n",
    "\n",
    "\n",
    "#Report results\n",
    "\n",
    "print('Optimal hyperparameter for Ridge Regression:')\n",
    "print('Degree %d polynomial expansion' % d_ridge_value)\n",
    "print('Lambda = %.4f' % lam_ridge_value)\n",
    "print('MSE  = %.4f' % mse_ridge_value)\n",
    "\n",
    "print('\\n\\nOptimal hyperparameter for Lasso Regression:')\n",
    "print('Degree %d polynomial expansion' % d_lasso_value)\n",
    "print('Lambda = %.4f' % lam_lasso_value)\n",
    "print('MSE  = %.4f\\n' % mse_lasso_value)\n",
    "\n",
    "\n",
    "print(\"MSE when the optimal hyperparameters are applied:\")\n",
    "print('\\nFor Ridge Regression with using degree %d polynomial expansion and lambda = %.4f' % (d_ridge_value, lam_ridge_value))\n",
    "print(\"Ridge MSE: %.4f\" % mse_ridge)\n",
    "print('\\n\\nFor Lasso with using degree %d polynomial expansion and lambda = %.4f' % (d_lasso_value, lam_lasso_value))\n",
    "print(\"Lasso MSE: %.4f\\n\" % mse_lasso)\n",
    "\n",
    "###################################################\n",
    "##### YOUR CODE ENDS HERE #########################\n",
    "###################################################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP96ktvsOI4PiuW52tcNLjx",
   "collapsed_sections": [],
   "name": "Practical1_starter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
